{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DS1** **heart Dieases**"
      ],
      "metadata": {
        "id": "fz_Pq7pObnAQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qTRlpsOKQkd3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from imblearn.over_sampling import SMOTE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Load and Inspect Heart Disease Dataset**"
      ],
      "metadata": {
        "id": "Drv2CDKDtiJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (make sure the file exists in your working directory)\n",
        "df = pd.read_csv(\"heart.csv\")\n",
        "\n",
        "# Show structure of dataset\n",
        "print(df.info())\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipuu4oaEQ7zw",
        "outputId": "ca179473-b351-4b36-b306-c27628ba07cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1025 entries, 0 to 1024\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       1025 non-null   int64  \n",
            " 1   sex       1025 non-null   int64  \n",
            " 2   cp        1025 non-null   int64  \n",
            " 3   trestbps  1025 non-null   int64  \n",
            " 4   chol      1025 non-null   int64  \n",
            " 5   fbs       1025 non-null   int64  \n",
            " 6   restecg   1025 non-null   int64  \n",
            " 7   thalach   1025 non-null   int64  \n",
            " 8   exang     1025 non-null   int64  \n",
            " 9   oldpeak   1025 non-null   float64\n",
            " 10  slope     1025 non-null   int64  \n",
            " 11  ca        1025 non-null   int64  \n",
            " 12  thal      1025 non-null   int64  \n",
            " 13  target    1025 non-null   int64  \n",
            "dtypes: float64(1), int64(13)\n",
            "memory usage: 112.2 KB\n",
            "None\n",
            "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
            "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
            "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
            "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
            "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
            "\n",
            "   ca  thal  target  \n",
            "0   2     3       0  \n",
            "1   0     3       0  \n",
            "2   0     3       0  \n",
            "3   1     3       0  \n",
            "4   3     2       0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Normalize and Apply Clustering (Unsupervised Learning)**"
      ],
      "metadata": {
        "id": "Ld2Yb6g5tmsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df.drop(\"target\", axis=1))  # exclude label for unsupervised learning\n",
        "\n",
        "# KMeans clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
        "sil_kmeans = silhouette_score(X_scaled, kmeans_labels)\n",
        "\n",
        "# DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Handle case where DBSCAN assigns -1 (noise) to many points\n",
        "try:\n",
        "    sil_dbscan = silhouette_score(X_scaled, dbscan_labels)\n",
        "except:\n",
        "    sil_dbscan = \"N/A (DBSCAN failed or all noise)\"\n",
        "\n",
        "print(\"Silhouette Score (KMeans):\", sil_kmeans)\n",
        "print(\"Silhouette Score (DBSCAN):\", sil_dbscan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA3DgeBiQ-0m",
        "outputId": "042c9921-af11-466d-a1a4-82aa87d0b6f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score (KMeans): 0.16636832597727694\n",
            "Silhouette Score (DBSCAN): -0.1597353544931689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Feature Selection (Supervised Learning)**"
      ],
      "metadata": {
        "id": "_4QzWz9ftr8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=5)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(\"Selected Features:\", selected_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC9HLADSRCTI",
        "outputId": "70fbe98b-5d21-4c92-990c-8770aa25fecd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features: Index(['cp', 'thalach', 'exang', 'oldpeak', 'ca'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Handle Class Imbalance with SMOTE**"
      ],
      "metadata": {
        "id": "lRtQVltpt1J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X[selected_features], y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check minimum class count for safe SMOTE\n",
        "class_counts = np.unique(y_train, return_counts=True)[1]\n",
        "min_class_count = min(class_counts)\n",
        "k_neighbors = min(5, min_class_count - 1)\n",
        "\n",
        "smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Original Class Distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
        "print(\"Balanced Class Distribution:\", dict(zip(*np.unique(y_train_bal, return_counts=True))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyv9dr9QRFQB",
        "outputId": "8183c1ef-69dd-4fd3-b77e-6ced880c2093"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Class Distribution: {np.int64(0): np.int64(397), np.int64(1): np.int64(423)}\n",
            "Balanced Class Distribution: {np.int64(0): np.int64(423), np.int64(1): np.int64(423)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Train and Evaluate Multiple Classifiers**"
      ],
      "metadata": {
        "id": "_e6h-XWMt7ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier()\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_bal, y_train_bal)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"\\n {name}\")\n",
        "    print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr7Dh4DVRJR7",
        "outputId": "c75112f7-d2e8-4dbf-c57c-2860184d3e18"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Logistic Regression\n",
            "Accuracy: 0.7804878048780488\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.70      0.76       102\n",
            "           1       0.74      0.86      0.80       103\n",
            "\n",
            "    accuracy                           0.78       205\n",
            "   macro avg       0.79      0.78      0.78       205\n",
            "weighted avg       0.79      0.78      0.78       205\n",
            "\n",
            "Confusion Matrix:\n",
            " [[71 31]\n",
            " [14 89]]\n",
            "\n",
            " Decision Tree\n",
            "Accuracy: 0.9804878048780488\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       102\n",
            "           1       0.99      0.97      0.98       103\n",
            "\n",
            "    accuracy                           0.98       205\n",
            "   macro avg       0.98      0.98      0.98       205\n",
            "weighted avg       0.98      0.98      0.98       205\n",
            "\n",
            "Confusion Matrix:\n",
            " [[101   1]\n",
            " [  3 100]]\n",
            "\n",
            " Random Forest\n",
            "Accuracy: 0.9804878048780488\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       102\n",
            "           1       0.99      0.97      0.98       103\n",
            "\n",
            "    accuracy                           0.98       205\n",
            "   macro avg       0.98      0.98      0.98       205\n",
            "weighted avg       0.98      0.98      0.98       205\n",
            "\n",
            "Confusion Matrix:\n",
            " [[101   1]\n",
            " [  3 100]]\n",
            "\n",
            " Gradient Boosting\n",
            "Accuracy: 0.8634146341463415\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.80      0.85       102\n",
            "           1       0.83      0.92      0.87       103\n",
            "\n",
            "    accuracy                           0.86       205\n",
            "   macro avg       0.87      0.86      0.86       205\n",
            "weighted avg       0.87      0.86      0.86       205\n",
            "\n",
            "Confusion Matrix:\n",
            " [[82 20]\n",
            " [ 8 95]]\n",
            "\n",
            " AdaBoost\n",
            "Accuracy: 0.8\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.79      0.80       102\n",
            "           1       0.80      0.81      0.80       103\n",
            "\n",
            "    accuracy                           0.80       205\n",
            "   macro avg       0.80      0.80      0.80       205\n",
            "weighted avg       0.80      0.80      0.80       205\n",
            "\n",
            "Confusion Matrix:\n",
            " [[81 21]\n",
            " [20 83]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DS2   Students Performance**"
      ],
      "metadata": {
        "id": "MZzKqVLjRWVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Load and Explore Dataset\n",
        "import pandas as pd\n",
        "ds2 = pd.read_csv('Students Performance.csv')\n",
        "ds2.head()\n",
        "\n",
        "# Cell 2: Check for Nulls & Basic Info\n",
        "ds2.info()\n",
        "ds2.isnull().sum()\n",
        "\n",
        "# Cell 3: Encoding Categorical Data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "for col in ds2.columns:\n",
        "    if ds2[col].dtype == 'object':\n",
        "        ds2[col] = le.fit_transform(ds2[col])\n",
        "ds2.head()\n",
        "\n",
        "# Cell 4: Feature Extraction Techniques (PCA and LDA)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Define features and target\n",
        "X = ds2.drop('Math_Score', axis=1)\n",
        "y = ds2['Math_Score']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# LDA - create classes based on math score quantiles\n",
        "y_class = pd.qcut(y, q=3, labels=[0, 1, 2])\n",
        "lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "X_lda = lda.fit_transform(X_scaled, y_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6oH7rYFRaJK",
        "outputId": "3914acfb-de3e-4fa4-ed83-26a2df09b038"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 399 entries, 0 to 398\n",
            "Data columns (total 5 columns):\n",
            " #   Column           Non-Null Count  Dtype\n",
            "---  ------           --------------  -----\n",
            " 0   Math_Score       399 non-null    int64\n",
            " 1   Reading_Score    399 non-null    int64\n",
            " 2   Writing_Score    399 non-null    int64\n",
            " 3   Placement_Score  399 non-null    int64\n",
            " 4   Club_Join_Date   399 non-null    int64\n",
            "dtypes: int64(5)\n",
            "memory usage: 15.7 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_class, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Classification Using PCA Features\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM-tVUBoVSFf",
        "outputId": "2311642e-8ab5-4bd3-f71a-5f74d55668a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Using PCA Features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.35      0.29        26\n",
            "           1       0.17      0.09      0.12        32\n",
            "           2       0.27      0.32      0.29        22\n",
            "\n",
            "    accuracy                           0.24        80\n",
            "   macro avg       0.23      0.25      0.23        80\n",
            "weighted avg       0.22      0.24      0.22        80\n",
            "\n",
            "[[ 9  9  8]\n",
            " [18  3 11]\n",
            " [ 9  6  7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_lda, y_class, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Classification Using LDA Features\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_8_NOrdVZ7C",
        "outputId": "1c1e89d5-4a1e-4a99-95d5-58b895bb28c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Using LDA Features\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.27      0.26        26\n",
            "           1       0.30      0.25      0.27        32\n",
            "           2       0.36      0.41      0.38        22\n",
            "\n",
            "    accuracy                           0.30        80\n",
            "   macro avg       0.30      0.31      0.30        80\n",
            "weighted avg       0.30      0.30      0.30        80\n",
            "\n",
            "[[ 7 12  7]\n",
            " [15  8  9]\n",
            " [ 6  7  9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DS3**  CreditCard Fraud Detection"
      ],
      "metadata": {
        "id": "Xi_pJpYIWFFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Load and Clean Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# Drop rows where target is missing (just in case)\n",
        "df = df.dropna(subset=['Class'])\n",
        "\n",
        "# Drop rows with any missing values in features\n",
        "df = df.dropna()\n",
        "\n",
        "# Display basic info\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"Missing values:\\n\", df.isnull().sum())\n",
        "print(\"Class distribution:\\n\", df['Class'].value_counts())\n",
        "\n",
        "# Cell 2: Define features and target\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Cell 3: Train-Test Split & Before Balancing Evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"\\n Before Balancing\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Cell 4: Apply SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"\\n After SMOTE Balancing\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Cell 5: Apply RandomUnderSampler\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_res, y_res = rus.fit_resample(X, y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"\\n After RandomUnderSampler\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDjVqAM-WSEM",
        "outputId": "bfcf9b13-c3fb-4588-97d0-491981201d3d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (284807, 31)\n",
            "Missing values:\n",
            " Time      0\n",
            "V1        0\n",
            "V2        0\n",
            "V3        0\n",
            "V4        0\n",
            "V5        0\n",
            "V6        0\n",
            "V7        0\n",
            "V8        0\n",
            "V9        0\n",
            "V10       0\n",
            "V11       0\n",
            "V12       0\n",
            "V13       0\n",
            "V14       0\n",
            "V15       0\n",
            "V16       0\n",
            "V17       0\n",
            "V18       0\n",
            "V19       0\n",
            "V20       0\n",
            "V21       0\n",
            "V22       0\n",
            "V23       0\n",
            "V24       0\n",
            "V25       0\n",
            "V26       0\n",
            "V27       0\n",
            "V28       0\n",
            "Amount    0\n",
            "Class     0\n",
            "dtype: int64\n",
            "Class distribution:\n",
            " Class\n",
            "0    284315\n",
            "1       492\n",
            "Name: count, dtype: int64\n",
            "\n",
            " Before Balancing\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.97      0.77      0.86        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.99      0.88      0.93     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n",
            "Confusion Matrix:\n",
            " [[56862     2]\n",
            " [   23    75]]\n",
            "\n",
            " After SMOTE Balancing\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56750\n",
            "           1       1.00      1.00      1.00     56976\n",
            "\n",
            "    accuracy                           1.00    113726\n",
            "   macro avg       1.00      1.00      1.00    113726\n",
            "weighted avg       1.00      1.00      1.00    113726\n",
            "\n",
            "Confusion Matrix:\n",
            " [[56739    11]\n",
            " [    0 56976]]\n",
            "\n",
            " After RandomUnderSampler\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.96      0.93        99\n",
            "           1       0.96      0.89      0.92        98\n",
            "\n",
            "    accuracy                           0.92       197\n",
            "   macro avg       0.93      0.92      0.92       197\n",
            "weighted avg       0.93      0.92      0.92       197\n",
            "\n",
            "Confusion Matrix:\n",
            " [[95  4]\n",
            " [11 87]]\n"
          ]
        }
      ]
    }
  ]
}